{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198dcb77-221a-4436-8eb8-441cafff4eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/noor/A/projects/Upstyle/.venv/bin/python\n",
      "Hello MindSpore!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"Hello MindSpore!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3441d2c9-dea0-45d0-a073-12182562cdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (1.19.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnx) (6.33.1)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnx) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnx) (0.5.4)\n",
      "Requirement already satisfied: onnxruntime in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (1.23.2)\n",
      "Requirement already satisfied: coloredlogs in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnxruntime) (6.33.1)\n",
      "Requirement already satisfied: sympy in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch torchvision --upgrade\n",
    "\n",
    "# Install Hugging Face transformers + tokenizers + huggingface_hub\n",
    "!pip install -q transformers tokenizers huggingface-hub --upgrade\n",
    "\n",
    "# ONNX + ONNX Runtime for validation\n",
    "!pip install -q onnx onnxruntime onnx-simplifier\n",
    "\n",
    "# Pillow for image I/O, and timm (if FashionCLIP uses timm backbones)\n",
    "!pip install -q pillow timm\n",
    "\n",
    "# Optional for debugging graphs\n",
    "!pip install -q graphviz\n",
    "\n",
    "# (Do not install MindSpore (large) here if you only need converter_lite)\n",
    "# You need to download MindSpore-Lite converter offline package (see later cell).\n",
    "\n",
    "!pip install onnx --upgrade\n",
    "!pip install onnxruntime --upgrade\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab128d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Python imports and utility functions\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer, CLIPVisionModel, CLIPTextModel\n",
    "import onnx\n",
    "import onnxruntime as ort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ea9c5b-004a-4eda-aa45-19ebce0324a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working?\n"
     ]
    }
   ],
   "source": [
    "print(\"working?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f42ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF FashionCLIP via Transformers (CLIPModel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HF_MODEL = \"patrickjohncyh/fashion-clip\"\n",
    "\n",
    "# Use CLIPModel / CLIPProcessor if HF model is compatible\n",
    "# If this errors, clone the GitHub repo and use model loading from there (code cell below).\n",
    "try:\n",
    "    model = CLIPModel.from_pretrained(HF_MODEL, dtype=torch.float32)  # loads vision + text encoders\n",
    "    processor = CLIPProcessor.from_pretrained(HF_MODEL)\n",
    "    model.eval()\n",
    "    print(\"Loaded HF FashionCLIP via Transformers (CLIPModel).\")\n",
    "except Exception as e:\n",
    "    print(\"Transformers CLIPModel load failed — error:\", e)\n",
    "    print(\"Fallback: clone the GitHub repo and use the provided model code (see README).\")\n",
    "    # If fallback is needed, you'll need to clone repo and import FashionCLIP class (not automated here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa0660c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': torch.Size([1, 3, 224, 224]), 'input_ids': torch.Size([2, 8]), 'attention_mask': torch.Size([2, 8])}\n",
      "PyTorch image_embeds.shape: torch.Size([1, 512])\n",
      "PyTorch text_embeds.shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Prepare inputs and run a forward pass in PyTorch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Example image: use any URL or local file. Here we use a tiny sample; replace with your dataset.\n",
    "img_url = \"/home/noor/A/projects/Upstyle/upstyle_ai/clothes_data/Hoodie8.jpg\"  # sample fashion-like image\n",
    "image = Image.open(img_url).convert(\"RGB\")\n",
    "\n",
    "# Example text prompt(s)\n",
    "texts = [\"a photo of a red dress\", \"black leather jacket\"]\n",
    "\n",
    "# Process inputs using the CLIPProcessor (handles transforms + tokenization)\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Inspect shapes\n",
    "print({k: v.shape for k, v in inputs.items()})\n",
    "\n",
    "# Run the model to get embeddings (PyTorch)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**{k: inputs[k] for k in [\"input_ids\", \"attention_mask\", \"pixel_values\"] if k in inputs})\n",
    "    # CLIPModel typically returns: vision_model_output, text_model_output, image_embeds, text_embeds, logits_per_image, logits_per_text\n",
    "    # fetch image and text embeddings\n",
    "    if hasattr(outputs, \"image_embeds\"):\n",
    "        image_embeds = outputs.image_embeds\n",
    "        text_embeds = outputs.text_embeds\n",
    "    else:\n",
    "        # Adapt if different return structure\n",
    "        image_embeds = outputs[1] if len(outputs) > 1 else outputs[0]\n",
    "        text_embeds = outputs[0] if len(outputs) > 0 else outputs[1]\n",
    "\n",
    "print(\"PyTorch image_embeds.shape:\", image_embeds.shape)\n",
    "print(\"PyTorch text_embeds.shape:\", text_embeds.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8948e7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionCLIPWrapper(\n",
       "  (clip): CLIPModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (position_embedding): Embedding(77, 512)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "        (position_embedding): Embedding(50, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 5 — Create a wrapper nn.Module for ONNX export that accepts (pixel_values, input_ids, attention_mask)\n",
    "# and returns normalized embeddings or logits (depending on FashionCLIP's forward signature).\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class FashionCLIPWrapper(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        # Use the HF CLIPModel forward and return the image & text embeddings\n",
    "        outputs = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        # Standard transformers CLIPModel returns .image_embeds and .text_embeds\n",
    "        image_embeds = outputs.image_embeds\n",
    "        text_embeds = outputs.text_embeds\n",
    "        # Optionally L2-normalize embeddings as CLIP does for similarity computations\n",
    "        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "        # Return as tuple\n",
    "        return image_embeds, text_embeds\n",
    "\n",
    "# instantiate wrapper\n",
    "wrapper = FashionCLIPWrapper(model)\n",
    "wrapper.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53885d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values.shape: torch.Size([1, 3, 224, 224])\n",
      "input_ids.shape: torch.Size([2, 8]) attention_mask.shape: torch.Size([2, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15323/1416159311.py:32: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1118 15:28:53.307000 15323 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 13 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `FashionCLIPWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `FashionCLIPWrapper([...]` with `torch.export.export(..., strict=False)`... ❌\n",
      "[torch.onnx] Obtain model graph for `FashionCLIPWrapper([...]` with `torch.export.export(..., strict=True)`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] Error while creating guard:\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] Name: ''\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     Source: shape_env\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     Create Function: SHAPE_ENV\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     Guard Types: None\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     Code List: None\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     Object Weakref: None\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     Guarded Class Weakref: None\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] Traceback (most recent call last):\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/_guards.py\", line 366, in create\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return self.create_fn(builder, self)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/_dynamo/guards.py\", line 2548, in SHAPE_ENV\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     python_code_parts, verbose_code_parts = _get_code_parts(\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]                                             ^^^^^^^^^^^^^^^^\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/_dynamo/guards.py\", line 2522, in _get_code_parts\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return output_graph.shape_env.produce_guards_verbose(\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 5928, in produce_guards_verbose\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     raise ConstraintViolationError(\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] torch.fx.experimental.symbolic_shapes.ConstraintViolationError: Constraints violated (L['pixel_values'].size()[2], L['pixel_values'].size()[3])! For more information, run with TORCH_LOGS=\"+dynamic\".\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   - You marked L['pixel_values'].size()[2] as dynamic but your code specialized it to be a constant (224). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] \n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] User stack:\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/tmp/ipykernel_15323/3581413160.py\", line 12, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     outputs = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return forward_call(*args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     output = func(self, *args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py\", line 982, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return forward_call(*args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py\", line 742, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return forward_call(*args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py\", line 197, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] \n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   - You marked L['pixel_values'].size()[3] as dynamic but your code specialized it to be a constant (224). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] \n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] User stack:\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/tmp/ipykernel_15323/3581413160.py\", line 12, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     outputs = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return forward_call(*args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/utils/generic.py\", line 918, in wrapper\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     output = func(self, *args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py\", line 982, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return forward_call(*args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py\", line 742, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     return forward_call(*args, **kwargs)\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py\", line 197, in forward\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0]     if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n",
      "E1118 15:28:57.313000 15323 torch/_guards.py:368] [0/0] \n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0] Created at:\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 773, in trace_frame\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]     tracer = InstructionTranslator(\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3864, in __init__\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]     output=OutputGraph(\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 508, in __init__\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]     self.init_ambient_guards()\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]   File \"/home/noor/A/projects/Upstyle/.venv/lib/python3.11/site-packages/torch/_dynamo/output_graph.py\", line 668, in init_ambient_guards\n",
      "E1118 15:28:57.316000 15323 torch/_guards.py:370] [0/0]     self.guards.add(ShapeEnvSource().make_guard(GuardBuilder.SHAPE_ENV))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `FashionCLIPWrapper([...]` with `torch.export.export(..., strict=True)`... ❌\n",
      "ONNX export failed — check model structure / ops. Error: Failed to export the model with torch.export. \u001b[96mThis is step 1/3\u001b[0m of exporting the model to ONNX. Next steps:\n",
      "- Modify the model code for `torch.export.export` to succeed. Refer to https://pytorch.org/docs/stable/generated/exportdb/index.html for more information.\n",
      "- Debug `torch.export.export` and submit a PR to PyTorch.\n",
      "- Create an issue in the PyTorch GitHub repository against the \u001b[96m*torch.export*\u001b[0m component and attach the full error stack as well as reproduction scripts.\n",
      "\n",
      "## Exception summary\n",
      "\n",
      "<class 'torch._dynamo.exc.UserError'>: Constraints violated (L['pixel_values'].size()[2], L['pixel_values'].size()[3])! For more information, run with TORCH_LOGS=\"+dynamic\".\n",
      "  - You marked L['pixel_values'].size()[2] as dynamic but your code specialized it to be a constant (224). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.\n",
      "  - You marked L['pixel_values'].size()[3] as dynamic but your code specialized it to be a constant (224). If you're using mark_dynamic, either remove it or use maybe_mark_dynamic. If you're using Dim.DYNAMIC, replace it with either Dim.STATIC or Dim.AUTO.\n",
      "\n",
      "The error above occurred when calling torch.export.export. If you would like to view some more information about this error, and get a list of all other errors that may occur in your export call, you can replace your `export()` call with `draft_export()`.\n",
      "\n",
      "(Refer to the full stack trace above for more information.)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install onnxscript onnx onnxruntime --upgrade\n",
    "\n",
    "# Cell 6 — Export the wrapper to ONNX\n",
    "import torch\n",
    "\n",
    "# Construct dummy inputs matching real shapes\n",
    "# pixel_values: (batch_size, 3, H, W)\n",
    "batch_size = 1\n",
    "pixel_values = inputs[\"pixel_values\"]  # from processor earlier (tensor)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# If processor returned batch dimension mismatch, ensure shapes:\n",
    "print(\"pixel_values.shape:\", pixel_values.shape)\n",
    "print(\"input_ids.shape:\", input_ids.shape, \"attention_mask.shape:\", attention_mask.shape)\n",
    "\n",
    "onnx_path = \"fashionclip.onnx\"\n",
    "\n",
    "# Set export options\n",
    "# - opset_version: 13 or higher recommended\n",
    "# - dynamic_axes: batch size dynamic and text sequence length dynamic\n",
    "dynamic_axes = {\n",
    "    \"pixel_values\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
    "    \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "    \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "    \"image_embeds\": {0: \"batch_size\"},\n",
    "    \"text_embeds\": {0: \"batch_size\", 1: \"seq_len\"}  # text_embeds may have shape (batch, embed_dim) - adjust if needed\n",
    "}\n",
    "\n",
    "# ONNX export requires a tuple of inputs; ensure wrapper signature matches export\n",
    "try:\n",
    "    torch.onnx.export(\n",
    "        wrapper,\n",
    "        (pixel_values, input_ids, attention_mask),\n",
    "        onnx_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"pixel_values\", \"input_ids\", \"attention_mask\"],\n",
    "        output_names=[\"image_embeds\", \"text_embeds\"],\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "    print(\"ONNX export succeeded:\", onnx_path)\n",
    "except Exception as e:\n",
    "    print(\"ONNX export failed — check model structure / ops. Error:\", e)\n",
    "    # Useful debugging: try torch.onnx.export with verbose=True or export smaller components (vision/text separately).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c124724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved simplified ONNX to: fashionclip.simplified.onnx\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Simplify ONNX model (helps with compatibility for some converters)\n",
    "# Requires onnx-simplifier\n",
    "from onnxsim import simplify\n",
    "\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "model_simp, check = simplify(onnx_model)\n",
    "if check:\n",
    "    simplified_path = \"fashionclip.simplified.onnx\"\n",
    "    onnx.save(model_simp, simplified_path)\n",
    "    print(\"Saved simplified ONNX to:\", simplified_path)\n",
    "    onnx_path = simplified_path\n",
    "else:\n",
    "    print(\"ONNX simplification failed or returned not checkable. Keeping original ONNX.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c26a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity image embedding (PyTorch vs ONNX): 0.9999999303953517\n",
      "Cosine similarity text embedding (PyTorch vs ONNX): 1.0000000546046477\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Validate parity: run ONNXRuntime and compare to PyTorch outputs (cosine similarity)\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Load ONNX model with ONNX Runtime\n",
    "ort_session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Prepare inputs as numpy arrays\n",
    "ort_inputs = {\n",
    "    \"pixel_values\": pixel_values.cpu().numpy(),\n",
    "    \"input_ids\": input_ids.cpu().numpy(),\n",
    "    \"attention_mask\": attention_mask.cpu().numpy(),\n",
    "}\n",
    "\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "onnx_image_embeds = ort_outs[0]\n",
    "onnx_text_embeds = ort_outs[1]\n",
    "\n",
    "# Convert torch embeddings to numpy\n",
    "pt_image = image_embeds.cpu().numpy()\n",
    "pt_text = text_embeds.cpu().numpy()\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-8)\n",
    "\n",
    "print(\"Cosine similarity image embedding (PyTorch vs ONNX):\",\n",
    "      cos_sim(pt_image, onnx_image_embeds))\n",
    "print(\"Cosine similarity text embedding (PyTorch vs ONNX):\",\n",
    "      cos_sim(pt_text, onnx_text_embeds))\n",
    "# Expect values close to 1.0 (small numerical differences OK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "962edd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in ./upstyle_venv/lib/python3.11/site-packages (1.19.1)\n",
      "Requirement already satisfied: onnxruntime in ./upstyle_venv/lib/python3.11/site-packages (1.23.2)\n",
      "Requirement already satisfied: numpy in ./upstyle_venv/lib/python3.11/site-packages (2.3.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in ./upstyle_venv/lib/python3.11/site-packages (from onnx) (5.29.5)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in ./upstyle_venv/lib/python3.11/site-packages (from onnx) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in ./upstyle_venv/lib/python3.11/site-packages (from onnx) (0.5.3)\n",
      "Requirement already satisfied: coloredlogs in ./upstyle_venv/lib/python3.11/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./upstyle_venv/lib/python3.11/site-packages (from onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: packaging in ./upstyle_venv/lib/python3.11/site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: sympy in ./upstyle_venv/lib/python3.11/site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./upstyle_venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./upstyle_venv/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting tokenizers==0.15.0\n",
      "  Using cached tokenizers-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: mindspore==2.7.1 in ./upstyle_venv/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: mindformers==1.3.0 in ./upstyle_venv/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in ./upstyle_venv/lib/python3.11/site-packages (from tokenizers==0.15.0) (0.36.0)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (5.29.5)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (3.0.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (12.0.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (1.16.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (25.0)\n",
      "Requirement already satisfied: psutil>=5.6.1 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (7.1.3)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (1.6.3)\n",
      "Requirement already satisfied: safetensors>=0.4.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (0.6.2)\n",
      "Requirement already satisfied: dill>=0.3.7 in ./upstyle_venv/lib/python3.11/site-packages (from mindspore==2.7.1) (0.3.8)\n",
      "Requirement already satisfied: setuptools in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (65.5.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (0.2.1)\n",
      "Requirement already satisfied: ftfy>=6.1.1 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (6.3.1)\n",
      "Requirement already satisfied: regex>=2022.10.31 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=6.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (6.0.3)\n",
      "Requirement already satisfied: jieba>=0.42.1 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese>=1.0.3 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (1.0.3)\n",
      "Requirement already satisfied: nltk>=2.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (3.9.1)\n",
      "Requirement already satisfied: mindpet==1.0.4 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (1.0.4)\n",
      "Requirement already satisfied: mdtex2html in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (1.3.1)\n",
      "Requirement already satisfied: opencv-python-headless in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (4.11.0.86)\n",
      "Requirement already satisfied: pyarrow==12.0.1 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (12.0.1)\n",
      "Requirement already satisfied: datasets==2.18.0 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (2.18.0)\n",
      "Requirement already satisfied: tiktoken in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (3.1.6)\n",
      "Requirement already satisfied: setproctitle in ./upstyle_venv/lib/python3.11/site-packages (from mindformers==1.3.0) (1.3.7)\n",
      "Requirement already satisfied: filelock in ./upstyle_venv/lib/python3.11/site-packages (from datasets==2.18.0->mindformers==1.3.0) (3.20.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./upstyle_venv/lib/python3.11/site-packages (from datasets==2.18.0->mindformers==1.3.0) (0.7)\n",
      "Requirement already satisfied: pandas in ./upstyle_venv/lib/python3.11/site-packages (from datasets==2.18.0->mindformers==1.3.0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./upstyle_venv/lib/python3.11/site-packages (from datasets==2.18.0->mindformers==1.3.0) (2.32.5)\n",
      "Requirement already satisfied: xxhash in ./upstyle_venv/lib/python3.11/site-packages (from datasets==2.18.0->mindformers==1.3.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in ./upstyle_venv/lib/python3.11/site-packages (from datasets==2.18.0->mindformers==1.3.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in ./upstyle_venv/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->mindformers==1.3.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in ./upstyle_venv/lib/python3.11/site-packages (from datasets==2.18.0->mindformers==1.3.0) (3.13.2)\n",
      "Requirement already satisfied: click in ./upstyle_venv/lib/python3.11/site-packages (from mindpet==1.0.4->mindformers==1.3.0) (8.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./upstyle_venv/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./upstyle_venv/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./upstyle_venv/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->mindformers==1.3.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./upstyle_venv/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->mindformers==1.3.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./upstyle_venv/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->mindformers==1.3.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./upstyle_venv/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->mindformers==1.3.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./upstyle_venv/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->mindformers==1.3.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./upstyle_venv/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->mindformers==1.3.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./upstyle_venv/lib/python3.11/site-packages (from aiohttp->datasets==2.18.0->mindformers==1.3.0) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./upstyle_venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.18.0->mindformers==1.3.0) (3.11)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./upstyle_venv/lib/python3.11/site-packages (from astunparse>=1.6.3->mindspore==2.7.1) (0.46.1)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in ./upstyle_venv/lib/python3.11/site-packages (from astunparse>=1.6.3->mindspore==2.7.1) (1.17.0)\n",
      "Requirement already satisfied: wcwidth in ./upstyle_venv/lib/python3.11/site-packages (from ftfy>=6.1.1->mindformers==1.3.0) (0.2.14)\n",
      "Requirement already satisfied: joblib in ./upstyle_venv/lib/python3.11/site-packages (from nltk>=2.0->mindformers==1.3.0) (1.5.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./upstyle_venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.18.0->mindformers==1.3.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./upstyle_venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.18.0->mindformers==1.3.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./upstyle_venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.18.0->mindformers==1.3.0) (2025.10.5)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy>=1.5.4 (from mindspore==2.7.1)\n",
      "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "  Using cached scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./upstyle_venv/lib/python3.11/site-packages (from jinja2->mindformers==1.3.0) (3.0.3)\n",
      "Requirement already satisfied: markdown in ./upstyle_venv/lib/python3.11/site-packages (from mdtex2html->mindformers==1.3.0) (3.10)\n",
      "Requirement already satisfied: latex2mathml in ./upstyle_venv/lib/python3.11/site-packages (from mdtex2html->mindformers==1.3.0) (3.78.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./upstyle_venv/lib/python3.11/site-packages (from pandas->datasets==2.18.0->mindformers==1.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./upstyle_venv/lib/python3.11/site-packages (from pandas->datasets==2.18.0->mindformers==1.3.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./upstyle_venv/lib/python3.11/site-packages (from pandas->datasets==2.18.0->mindformers==1.3.0) (2025.2)\n",
      "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, tokenizers\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.3.4\n",
      "\u001b[2K    Uninstalling numpy-2.3.4:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.4\n",
      "\u001b[2K  Attempting uninstall: scipy━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.16.3━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.16.3:━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.16.37m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K  Attempting uninstall: tokenizers\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.19.1\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling tokenizers-0.19.1:[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.19.10m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [tokenizers] \u001b[32m1/3\u001b[0m [scipy]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.3.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
      "transformers 4.40.0 requires tokenizers<0.20,>=0.19, but you have tokenizers 0.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.23.5 scipy-1.15.3 tokenizers-0.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: mindspore_lite-2.3.0rc1-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime numpy --upgrade\n",
    "!pip install numpy==1.23.5 tokenizers==0.15.0 mindspore==2.7.1 mindformers==1.3.0\n",
    "\n",
    "# MindSpore Lite Python wheel (CPU)\n",
    "# Example for Linux (adjust version if needed)\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.3.0-rc1/MindSpore/lite/release/linux/cpu/x86_64/mindspore_lite-2.3.0rc1-cp310-cp310-linux_x86_64.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97472003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Upstyle MindSpore",
   "language": "python",
   "name": "upstyle-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
